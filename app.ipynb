{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df611c16-3644-46b9-a9ae-019a9a87a95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e577359-2837-4020-aae9-b9b2c7733e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import fitz\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdc0726e-0a6b-4029-8c7c-9ca031e5a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override = True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48accf9e-adfe-4998-b2f3-b1cbb1778979",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = r\"C:\\Users\\User\\Documents\\Projects\\llm_engineering\\PDF-chatbot\\books\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b209bf52-8896-4bce-b028-06afa4a6971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text():\n",
    "    all_text = []\n",
    "    for filename in os.listdir(r\"C:\\Users\\User\\Documents\\Projects\\llm_engineering\\PDF-chatbot\\books\"):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text = ''\n",
    "            for page in doc:\n",
    "                content = page.get_text('text')\n",
    "                text += content\n",
    "            all_text.append(text)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size =500, chunk_overlap=100)\n",
    "    docs = text_splitter.create_documents(all_text)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = Chroma.from_documents(docs, embeddings)   \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7b593-9fac-4636-b92d-78ef66d7f052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c069d98f-67e8-4909-94ed-b84e8185a1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68f4ff53-7e1d-45bd-9adc-9f010787a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16b3af3b-1165-43cb-9af0-ac8d17d5a280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e74b70c8-935b-4ea1-89f1-93e803aee59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bot(query):\n",
    "    llms = ChatOpenAI(model = 'gpt-3.5-turbo')\n",
    "    rag_chain = RetrievalQA.from_chain_type(llm=llms, retriever =vector_store.as_retriever())\n",
    "    talk = rag_chain.invoke(query)\n",
    "    response = talk['result']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78a8f2b6-336f-4886-9f85-f70278b57fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_prompt_func(response):\n",
    "    system_prompt = 'you are an AI assitant who is very  knowlegeable and smart \\n'\n",
    "    system_prompt += f'You are to compare the user input and the response. this is the response:  {response}\\n'\n",
    "    system_prompt += \"if the response does not fit the user's needs, find a better answer. If the response is good, respond with just the same answer\"\n",
    "    return system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6977f9e-906c-48b4-ba92-6202adbd4d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"you are an AI assitant who is very  knowlegeable and smart \\nYou are to compare the user input and the response. this is the response:  I dont know\\nif the response does not fit the user's needs, find a better answer. If the response is good, respond with just the same answer\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt_func('I dont know')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5969700e-2833-4a0e-adc9-a4ba9e0330b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce2ce0-198c-4f24-a0e9-4537d4250404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47cd7cbe-ceaf-4d61-8c2a-13d69d984dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_func(query,response):\n",
    "    user_prompt = \"compare the query and the response and confirm if the response is suitable. \\n\"\n",
    "    user_prompt = 'If it is not suitable provide a better answer'\n",
    "    user_prompt+= f'query : {query}, response: {response}'\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f77c0f96-5630-4ffd-9777-49255ed1a7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 00:36:09.260 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 00:36:09.524 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\User\\anaconda3\\envs\\llms\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-03-24 00:36:09.524 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 00:36:09.524 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 00:36:09.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.title(\"Multi-PDF Chatbot for Exam Prep\")\n",
    "st.markdown(\"Ask anything from the loaded PDFs or infact, anything else...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a21589-a6aa-46cb-8a75-363eae295a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "640a1a49-07bf-4077-8471-4475695cc1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 00:44:25.814 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 00:44:25.814 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 00:44:25.814 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 00:44:25.814 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 00:44:25.831 Session state does not function when running a script without `streamlit run`\n",
      "2025-03-24 00:44:25.832 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 00:44:25.836 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "query = st.text_input('what do you need help with?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20664ab3-3d60-4764-b561-5cebb51a6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assistant():\n",
    "    response = bot(query)\n",
    "    chat_bot = openai.chat.completions.create(\n",
    "         model='gpt-3.5-turbo',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt_func(response)},\n",
    "            {\"role\": \"user\", \"content\": user_prompt_func(query,response)}\n",
    "      ],\n",
    "    )\n",
    "    result = chat_bot.choices[0].message.content\n",
    "    st.write(result)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "117fa93a-bdbc-45ce-9547-974c31c92dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "what do you need help with? who is responsible for rad 420\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The response is already suitable and accurate. Professor C. C. Ohagwu from the Department of Radiography and Radiological Sciences at Nnamdi Azikiwe University is responsible for RAD 420, which involves radiographs of the spine."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79094d-c397-4478-ae63-331e93e34af0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
